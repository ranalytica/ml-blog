---
title: "CVPR 2020: A snapshot"
excerpt: "The first virtual CVPR conference ended, with 1467 papers accepted, 29 tutorials, 64 workshops, and 7k virtual attendees. In this
blog post, I present a biased snapshot of the conference by summarizing some papers that caught my attention."
date: 2020-06-17 8:00:00
published: false
tags: 
  - computer-vision
  - deep-learning
  - conference
---

The first virtual CVPR conference ended, with 1467 papers accepted, 29 tutorials, 64 workshops, and 7k virtual attendees. The huge number of papers and the new virtual version made navigating the conference overwhelming (and very slow) at times. To get a grasp of the general trends of the conference this year, I will present in this blog post a biased snapshot of the conference by summarizing some papers that grabbed my attention.

*Disclaimer: This post is not a representation of the papers and subjects presented in CVPR; it is just a personnel overview of what I found interesting. Any feedback is welcomed!*



# CVPR 2020 in numbers
The statistics presented in this section are taken from the official [Opening & Awards](https://www.youtube.com/watch?v=aHUYXtbwl_8&t=1903s) presentation. Let's start
by some general statistics:

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/participants.png' | absolute_url }}">
</figure>

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/acceptance.png' | absolute_url }}">
</figure>

The trends of earlier years continued with a 20% increase in authors and a 29% increase in submitted papers, joined by the rising of the number of reviewers and area chairs to accommodate this expansion.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/authors_distribution.png' | absolute_url }}">
</figure>

Similar to last year, China is the first contributor to CVPR in terms of accepted papers with Tsinghua University with the most significant number of authors, followed by the USA as the second contributor by country and Google by the organization.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/subject_areas.png' | absolute_url }}">
</figure>

As expected, the majority of the accepted papers focus on topics related to learning, recognition, detection, and understanding. However, there is an increasing interest in relatively new areas such as label-efficient methods (e.g., transfer learning), image synthesis and robotic perception. Some emerging topics like fairness and explain AI are also starting to gather more attention within the computer vision community.


# Generative models and image synthesis

#### Learning Physics-Guided Face Relighting Under Directional Light ([paper](https://arxiv.org/abs/1906.03355), [CVPR virtual](http://cvpr20.com/event/learning-physics-guided-face-relighting-under-directional-light/)) 

Relighting involves relighting an unseen source image with its corresponding directional light, towards the desired directional light. The previous works
give good results but are limited to smooth lighting and do not model non-diffuse effects such as cast shadows and specularities.

To be able to create precise and believable relighting results and generalizes to complex illumination conditions and challenging poses, the authors propose
an end-to-end deep learning architecture that both delights and relights an image of a human face. This is done in two stages, as shown below.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/relighting.png' | absolute_url }}">
</figure>

The first stage consists of predicting the [albedo](https://en.wikipedia.org/wiki/Albedo) and [normals](https://en.wikipedia.org/wiki/Normal_(geometry)) of the input
image using a Unet architecture, the desired directional light in then used with the normal to predict the shading and then the [diffuse relighting](http://learnwebgl.brown37.net/09_lights/lights_diffuse.html). The outputs of the first stage are used in the second stage to predict the correct shading. The whole model is trained end-to-end with a generative adversarial network (GaN) loss similar to the one used in [pix2pix](https://arxiv.org/abs/1611.07004) paper.


#### SynSin: End-to-End View Synthesis From a Single Image ([paper](https://arxiv.org/abs/1912.08804), [CVPR virtual](http://cvpr20.com/event/synsin-end-to-end-view-synthesis-from-a-single-image-2/))

The goal of view synthesis is to generate new views of a scene given one or more images. But this can be challenging, requiring an understanding of the 3D scene from images, to overcome this, current methods rely on multiple images, train on ground-truth depth, or are limited to synthetic data. The authors propose a novel end-to-end model for view synthesis from a single image at test time while being trained on real images without any ground-truth 3D information (e.g., depth).

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/synsin.png' | absolute_url }}">
</figure>

SynSin takes an input image, the target image, and the desired relative pose (i.e., rotation and translation). The input image is first passed through a feature network to embed it into a feature space at each pixel location, followed by depth prediction at each pixel via a depth regressor. Based on the features and the depth information, a [point cloud](https://en.wikipedia.org/wiki/Point_cloud) representation is created, the relative pose (i.e., applying rotation and translation) is then used to render the features at the new view with a fully differentiable neural point cloud renderer. However, the projected features might have some artifacts, in order to fix this, a generator is used to fill the missing regions. The whole model is then trained end-to-end with: an L2 loss, a discriminator loss, and a [perceptual loss](https://arxiv.org/abs/1603.08155), without requiring any depth information. At test time, the network takes an image and the target relative pose and outputs the image with the desired view.

#### Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths from a Monocular Camera ([paper](https://arxiv.org/abs/2004.01294), [CVPR virtual](http://cvpr20.com/event/oral-session-1-3/))

The objective in this paper is to synthesize an image from arbitrary views and times given a collection of images of a dynamic scene, i.e., a series of images  captured by a single monocular camera from many the locations (image bellow, left). The method can produce a novel view from an arbitrary location within the original range of locations (image bellow, middle), and can also produce the dynamic content that appeared across any views in different time (image bellow, right). This is done using a single camera, without requiring a multiview system or human-specific priors as previous methods. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/dynamic_scenes_sythesis.png' | absolute_url }}">
</figure>

The authors combine the depth from multiview stereo (DMV) with the depth from a single view (DSV) using depth fusion network with the help of the input image from the target view, producing a scale-invariant and a complete depth map. With geometrically consistent depths across views, a novel view can be synthesized using a self-supervised rendering network that produces a photorealistic image in the presence of missing data with an adversarial loss and a reconstruction loss.

<figure style="width: 70%" class="align-center">
  <img src="{{ 'images/CVPR20/dynamic_scenes_sythesis_net.png' | absolute_url }}">
</figure>

#### STEFANN: Scene Text Editor using Font Adaptive Neural Network ([paper](https://arxiv.org/abs/1903.01192), [CVPR virtual](http://cvpr20.com/event/stefann-scene-text-editor-using-font-adaptive-neural-network/))

This paper presents a method to directly modify text in an image at a character level while maintaining the same style. 
This is done in two steps. First, a network called FANnetto takes as input the source character we would like to modify and outputs the target character while
keeping structural consistency and the style of the source. Secondly, the coloring network, Colornet, takes the output of the first stage and the source character and colors the target character while reserving visual consistency. After doing this process for each character of the text, the characters are placed in the in-painted background while maintaining the correct spacing between characters. Below are some examples of the results from [the project's webpage](https://prasunroy.github.io/stefann/).

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/stefann.png' | absolute_url }}">
</figure>

#### MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation ([paper](https://arxiv.org/abs/1911.11758), [CVPR virtual](http://cvpr20.com/event/mixnmatch-multifactor-disentanglement-and-encoding-for-conditional-image-generation/))

MixNMatch is a conditionnal GaN capable of disentangling background, object pose, shape, and texture from real images with minimal supervision, i.e., bounding box annotations to model background. A trained model can then be used to arbitrarily combine the our factors to generate new images, including sketch2color, cartoon2img, and img2gif applications.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/MixNMatch.png' | absolute_url }}">
</figure>

Given a collection of images of a single object category, the model is trained to simultaneously encode background, object pose, shape, and texture factors associated with each images int oa disentangled latent code space, and then generate real looking image by combining latent factors from the disentangled code space. Four encoders are used to separately encode each latent code. Four different latent codes are then sampled and fed into the [FineGAN](https://arxiv.org/abs/1811.11155) generator to hierarchically generate images, the model is then trained four image-code pair discriminators optimize the encoders and generator, to match their joint image-code distributions.

#### StarGAN v2: Diverse Image Synthesis for Multiple Domains ([paper](https://arxiv.org/abs/1912.01865), [CVPR virtual](http://cvpr20.com/event/stargan-v2-diverse-image-synthesis-for-multiple-domains-2/))

The main objective in image-to-image translation (i.e., changing some attributes of an image, such as hair color) to increase the quality and the diversity of the generated images, while maintaining high scalability over multiple domains (i.e., a domain refers to set of images having the same attribute value, like black hair).
Given that existing methods address only one of these issues, resulting in either limited diversity or various models for all domains. StarGAN v2 tries to solves both issues simultaneously, using style codes instead of an explicit domain labels as in the first version of [StarGAN](https://arxiv.org/abs/1711.09020).

<figure style="width: 90%" class="align-center">
  <img src="{{ 'images/CVPR20/starganv2.png' | absolute_url }}">
</figure>

The StarGAN v2 model contains four modules: A generator that translates an input image into an output image with the desired domain-specific style code. A latent encoder (or a mapping network) that produces a style code for each domain, one of which is randomly selected during training. A style encoder extracts the style code of an image, allowing the generator to perform reference-guided image synthesis, and a discriminator that distinguishes between real and fake (R/F) images from multiple domains. All modules except the generator contain multiple output branches, one of which is selected when training the corresponding domain. The model is then trained using an adversarial loss, a style reconstruction to force the generator to utilize the style code when generating the image, a style diversification loss to enable the generator to produce diverse images and a cycle loss to preserve the characteristics of each domain.

#### GAN Compression: Efficient Architectures for Interactive Conditional GANs ([paper](), [CVPR virtual]())

Conditional GANs (cGANs) give the ability to do controllable image synthesis for many computer vision and graphics applications. However, 
the computational resources needed for training them are orders of magnitude larger than that of traditional CNNs used for detection and recognition.
For example, GANs require 10x to 500x more computation that image recognition models. To solve this problem, the authors a GAN compression approach based on distillation, channel pruning, and neural architecture search (NAS), resulting in a compressed model while maintaining the same performance.

<figure style="width: 90%" class="align-center">
  <img src="{{ 'images/CVPR20/gan_compression.png' | absolute_url }}">
</figure>

The proposed GAN Compression framework takes a pre-trained generator, considered as a teacher, to be distilled into a smaller *once-for-all* student generator that contains all possible channel numbers through weight sharing, where different channel numbers are chosen for the student at each iteration.
Now, in order to choose the correct number of channels of the student for each layer, many sub-generators are extracted from the *once-for-all* generator and evaluated, creating the candidate generator pool. Finally, the best sub-generator with the desired compression ratio target and performance target (e.g., [FID](https://arxiv.org/abs/1706.08500) or mIoU) using [one-shot NAS](https://arxiv.org/abs/1904.00420), the selected generator is then fine-tuned, resulting in the final compressed model.

#### Semantic Pyramid for Image Generation ([paper](https://arxiv.org/abs/2003.06221), [CVPR virtual](http://cvpr20.com/event/semantic-pyramid-for-image-generation-2/))

Semantic Pyramid tries to bridge the gap between discriminative and generative models. This is done using a novel GAN-based model that utilizes the space of deep features learned by a pre-trained classification model. Given a set of features extracted from a reference image, the model generates diverse image samples, each with matching features at each semantic level of the classification model.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/semantic_pyramid.png' | absolute_url }}">
</figure>

Concretely, given a pretrained classification network, a GaN network is designed with a generator with similar architecture as the classification network. Each layer of the generator is trained to be conditioned on the previous layers, and the corresponding layers of the classification network. For example, conditioning the generator on the classification feature close to the input results in an image similar to the input image of the classification model, with the possibility of exploring the sup-space of such image by sampling different noise vectors, while conditioning on deeper layers results in a wider distribution of generated images. The model is trained with an adversarial loss to produce realistic images, a diversity loss to produce diverse images with different noises, and a reconstruction loss to match the features of the generated image to the reference image. Different regions of the image can be conditioned on different semantic level using a masking operation $$m$$, which can be used to semantically modify the image.

#### Analyzing and Improving the Image Quality of StyleGAN ([paper](https://arxiv.org/abs/1912.04958), [CVPR virtual](http://cvpr20.com/event/analyzing-and-improving-the-image-quality-of-stylegan/))

In the first version of [StyleGAN](https://arxiv.org/abs/1812.04948), the authors proposed an alternative generator architecture, producing a high-quality image 
and capable of separating high-level attributes (e.g., pose and identity when trained on human faces). This new architecture consisted of using a mapping network from the latent space $$\mathcal{Z}$$ into an intermediate space $$\mathcal{W}$$ to more closely match the distribution of features in the training set, and avoid the forbidden combinations present in $$\mathcal{Z}$$. The intermediate latent vector is incorporated into the generator using Adaptive Instance Normalization ([AdaIN](https://arxiv.org/abs/1703.06868)) layers while a uniform noise is additively injected before each application of AdaIN, and trained in a [progressive](https://arxiv.org/abs/1710.10196) manner. Yeilding impressive results in data-driven unconditional generative image modeling. However, the generated images still contain some artifacts, like water-splotches (more details: [whichfaceisreal](http://www.whichfaceisreal.com/learn.html)) and unchanged positions of face attributes like eyes.

First, to avoid the droplet effects, which are results of the AdaIN discarding information in feature maps, AdaIN is replaced with a weight demodulation layer by removing some redundant operations, moving the addition of the noise to be outside of the active area of a style, and adjusting only the standard deviation per feature map. The progressive GaN training is removed to avoid the permanent positions of face attributes based on [MSG-GAN](https://arxiv.org/abs/1903.06048). Finally, StyleGAN2 introduces a new regularization term to the loss to enforce smoother latent space interpolations based on the Jacobian matrix at a single position at the intermediate latent space. 

#### Adversarial Latent Auto-encoders ([paper](https://arxiv.org/abs/2004.04467), [CVPR virtual](http://cvpr20.com/event/adversarial-latent-autoencoders/))

Auto-Encoders (AE) are characterized by their simplicity and their capability of combining generative and representational properties by learning an encoder-generator map simultaneously. However, they do not have the same generative capabilities as GaNs. The proposed Adversarial Latent Autoencoder (ALAE) retain the generative properties of GANs by learning an output data distribution with an adversarial strategy, with AE architecture where the latent distribution is learned from data to improve the disentanglement properties (i.e., the $$\mathcal{W}$$ intermediate latent space of StyleGAN).

<figure style="width: 70%" class="align-center">
  <img src="{{ 'images/CVPR20/ALAE.png' | absolute_url }}">
</figure>

The ALAE architecture decomposes the generator G and the discriminator D in two networks: F, G, and E, D, where the latent spaces between F and G, and between E and D are considered same, and refereed to as the intermediate latent space $$\mathcal{W}$$. In this case, the mapping network F is deterministic, while E and G are stochastic depending on an injected noise. The pair of networks (G,E) consist a generator-encoder network that auto-encodes the latent space $$\mathcal{W}$$, and trained to minimize the discrepancy $$\Delta$$ (e.g., an MSE loss) between the two distributions, i.e., the distribution at the input of G and the distribution of the output of E. As a whole, model is the trained by alternating between optimizing the GaN loss and the discrepancy $$\Delta$$.

#### Other papers:
- [Interpreting the Latent Space of GANs for Semantic Face Editing](https://arxiv.org/abs/1907.10786)
- [MaskGAN: Towards Diverse and Interactive Facial Image Manipulation](https://arxiv.org/abs/1907.11922)
- [Semantically Multi-modal Image Synthesis](https://arxiv.org/abs/2003.12697)
- [TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting](https://arxiv.org/abs/2003.14401)
- [Learning to Shadow Hand-drawn Sketches](https://arxiv.org/abs/2002.11812)
- [Wish You Were Here: Context-Aware Human Generation](https://arxiv.org/abs/2005.10663)
- [Disentangled Image Generation Through Structured Noise Injection](https://arxiv.org/abs/2004.12411)
- [MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks](https://arxiv.org/abs/1903.06048)
- [PatchVAE: Learning Local Latent Codes for Recognition](https://arxiv.org/abs/2004.03623)

# Recognition, Detection and Segmentation

#### PointRend: Image segmentation as rendering ([paper](https://arxiv.org/abs/1912.08193), [CVPR virtual](http://cvpr20.com/event/pointrend-image-segmentation-as-rendering/))

Image segmentation models, such as [Mask R-CNN](https://arxiv.org/abs/1703.06870), typically operate on regular grids: the input image is a regular grid of pixels, their hidden representations are feature vectors on a regular grid, and their outputs are label maps on a regular grid. However, a regular grid will unnecessarily over sample the smooth areas while simultaneously undersampling object boundaries, often resulting in blurry contours, as illustrated in the right figure below.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/CVPR20/pointrend.png' | absolute_url }}">
</figure>

The paper proposes to view image segmentation as a rendering problem and adapt classical ideas from computer graphics to render high-quality label maps efficiently. This is done using a neural network module called PointRend. PointRend takes as input a given number of CNN feature maps that are defined over regular grids and outputs high-resolution predictions over a finer grid. These fine predictions are only made in carefully selected points, chosen to be near high-frequency areas such as object boundaries where we have uncertain predictions (i.e., similar to [adaptive subdivision](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.3997&rep=rep1&type=pdf)), which are then upsampled and a small subhead is used to make the prediction from such point-wise features.

#### Self-training with Noisy Student improves ImageNet classification ([paper](https://arxiv.org/abs/1911.04252), [CVPR virtual](http://cvpr20.com/event/self-training-with-noisy-student-improves-imagenet-classification/))

Semi-supervised learning method work quite well in low-data regime, but with a large number of labeled data, fully-supervised learning still works best.
In this paper, the authors revisit this assumption and show that noisy self-training works well even when labeled data is abundant.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'images/CVPR20/noisy-self-training.png' | absolute_url }}">
</figure>

The method used a large corpus of unlabeled images (i.e., different than ImageNet training set distribution), and consists of three main steps. First a teacher model is trained on the labeled image, after than the teacher is used to generate pseudo labels on the unlabeled images, which are then used to train a student model on the combination of labeled images and pseudo labeled images, the student model is larger than the teacher and is trained with an injected noise (e.g., dropout). The student is then considered as a teacher and the last two step are repeated a few times to relabel the unlabeled data and training a new student. The last model achieves SOTA on ImageNet top-1 and shows a higher degree of robustness.

#### Designing network design spaces ([paper](https://arxiv.org/abs/2003.13678), [CVPR virtual](http://cvpr20.com/event/self-training-with-noisy-student-improves-imagenet-classification/))
#### EfficientDet: Scalable and Efficient Object Detection ([paper](https://arxiv.org/abs/1911.09070), [CVPR virtual]())
#### Dynamic Convolution: Attention Over Convolution Kernels ([paper](https://arxiv.org/abs/1912.03458))
#### Exploring Self-attention for Image Recognition ([paper](https://arxiv.org/abs/2004.13621), [CVPR virtual]())

#### Other papers:
- [Deep Snake for Real-Time Instance Segmentation](https://arxiv.org/abs/2001.01629)
- [Bridging the Gap Between Anchor-based and Anchor-free Detection](https://arxiv.org/abs/1912.02424)
- [SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization](https://arxiv.org/abs/1912.05027)
- [ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network](https://arxiv.org/abs/2002.10200)
- [Look-into-Object: Self-supervised Structure Modeling for Object Recognition](https://arxiv.org/abs/2003.14142)
- [Learning to Cluster Faces via Confidence and Connectivity Estimation](https://arxiv.org/abs/2004.00445)
- [PADS: Policy-Adapted Sampling for Visual Similarity Learning](https://arxiv.org/abs/2003.11113)
- [Evaluating Weakly Supervised Object Localization Methods Right](https://arxiv.org/abs/2001.07437)
- [Context R-CNN: Long Term Temporal Context for Per Camera Object Detection](https://arxiv.org/abs/1912.03538)
- [BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation](https://arxiv.org/abs/2001.00309)
- [Learning Dynamic Routing for Semantic Segmentation](https://arxiv.org/abs/2003.10401)
- [Hyperbolic Visual Embedding Learning for Zero-Shot Recognition](http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Hyperbolic_Visual_Embedding_Learning_for_Zero-Shot_Recognition_CVPR_2020_paper.pdf)
  


([paper](), [CVPR virtual]())

([paper](), [CVPR virtual]())

([paper](), [CVPR virtual]())

([paper](), [CVPR virtual]())